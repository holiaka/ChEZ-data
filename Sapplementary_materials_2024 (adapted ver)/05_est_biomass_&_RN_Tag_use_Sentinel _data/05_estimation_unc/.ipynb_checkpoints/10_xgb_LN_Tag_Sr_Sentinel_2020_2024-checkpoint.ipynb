{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-albany",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Est. of growing stock, biomass and RN content use UAV survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-optimum",
   "metadata": {},
   "source": [
    "# Load all data from .xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-permit",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import main libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f3bb1-bdaa-4890-8128-d0e0ba3c349b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Block warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-acquisition",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load biomass dataset\n",
    "site_data = './../../../05_input_data/Sites_with_Sentinel_bands_data_2024.xlsx'\n",
    "df = pd.read_excel(site_data, sheet_name='input_2020')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26fe86-c33c-45c2-a679-c88348d02063",
   "metadata": {},
   "source": [
    "# List of all parameters for est. of bioproductivity and RN content\n",
    "\"\"\"\n",
    "All parameters:\n",
    "'Site #', 'Origin', 'Dominant_s', 'LN_GS_cub_m', 'LN_M_stem_kg_m2', 'LN_M_stem_bark_kg_m2', 'LN_M_crown_kg_m2', 'LN_M_foliage_kg_m2', 'LN_M_AG_kg_m2', 'F_Soil', 'M_Soil', 'X_N36', 'Y_N36', 'Cs_wood_Bq/kg', 'Sr_wood_Bq/kg', 'LN_Cs_wood', 'LN_Sr_wood', 'Tag_Cs', 'Tag_Sr', 'LN_Tag_Cs', 'LN_Tag_Sr', 'Cs_2021_kB', 'Sr_2021_kB', 'AEDR_mean', 'r2019B01', 'r2019B02', 'r2019B03', 'r2019B04', 'r2019B05', 'r2019B06', 'r2019B07', 'r2019B08', 'r2019B09', 'r2019B10', 'r2019B11', 'r2019B12'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-mayor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting main working parameters\n",
    "select = pd.DataFrame(df, columns=[ 'Ln_Tag_Cs', 'Ln_Tag_Sr',\n",
    "                                  'r2020B01', 'r2020B02', 'r2020B03', 'r2020B04', 'r2020B05', 'r2020B06', 'r2020B07', 'r2020B08', 'r2020B09', 'r2020B10', 'r2020B11', 'r2020B12'])\n",
    "\n",
    "# Show the first five columns\n",
    "select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb489ae8-33d0-488c-80c0-792a62e6c213",
   "metadata": {},
   "source": [
    "# Pre-analysis of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-commission",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Information on all columns of data frame\n",
    "select.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-distributor",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discribe statistic of forest sites by species and origin\n",
    "df.groupby([\"Dominant_s\"])[\"LN_Tag_Cs\", \"LN_Tag_Sr\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-county",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics of main parameters\n",
    "select.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-imaging",
   "metadata": {},
   "source": [
    "# Selecting working columns for the XGBoost algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13ac08-283b-407e-b9cb-2f95ab11e985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choosing the predictor and independent variables\n",
    "predictor = \"LN_Tag_Sr\"\n",
    "indep_variables = ['r2020B04', 'r2020B07', 'r2020B08', 'r2020B09', 'r2020B10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac11171-8a4d-4686-a40f-7b111bde5134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete empty rows from the working column in a dataframe\n",
    "nan_value = float(\"NaN\")\n",
    "select.replace(\"\", nan_value, inplace=True)\n",
    "select.dropna(subset = [predictor], inplace=True) # IMPOTANT - input name of column\n",
    "print(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-burst",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We select independent variables and predicting parameter \n",
    "X = pd.DataFrame(select, columns=indep_variables)\n",
    "y = pd.DataFrame(select, columns=[predictor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-eleven",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns in \"X\" data collections\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-oakland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns in \"y\" data collections\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-colombia",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create first XGBoost model for estimate varibiality of output stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-registration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Importing the main library for building model and its analysis\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-trustee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# K-Folds cross-validation for estimation \"quality\" of input data for building model\n",
    "X_kfold = pd.DataFrame(X).to_numpy()\n",
    "y_kfold = pd.DataFrame(y).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-humor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for comparing datasets on homogeneity \n",
    "def display_scores(scores):\n",
    "    print(\"    Scores: {0}\\n    Mean: {1:.3f}\\n    Std: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-pilot",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output results of K-Folds cross-validation for XGBoost model\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in kfold.split(X_kfold):   \n",
    "    X_train, X_test = X_kfold[train_index], X_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "print('R square (R2): \\n')\n",
    "display_scores((scores))\n",
    "print('\\n', 20*'-')\n",
    "\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in kfold.split(X_kfold):   \n",
    "    X_train, X_test = X_kfold[train_index], X_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "      \n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "print('Root-mean-square error (RMSE): \\n')\n",
    "display_scores(np.sqrt(scores))\n",
    "print('\\n', 20*'-')\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in kfold.split(X_kfold):   \n",
    "    X_train, X_test = X_kfold[train_index], X_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    scores.append(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "print('Mean absolute error (MAE): \\n')\n",
    "display_scores((scores))\n",
    "print('\\n', 20*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-banana",
   "metadata": {},
   "source": [
    "# Add optimal hyperparameters for XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a5057-d093-40c7-a0d0-3ee7220483ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters data for the next step/stage\n",
    "p1 = 0.7318469753907232                  # colsample_bytree\n",
    "print('Colsample_bytree: ' + str(p1))\n",
    "\n",
    "p2 = 0.4550682668492081                 # gamma\n",
    "print('Gamma: ' + str(p2))\n",
    "\n",
    "p3 = 0.06644937736093733                 # learning_rate\n",
    "print('Learning_rate: ' + str(p3))\n",
    "\n",
    "p4 = 2            # max_depth\n",
    "print('Max_depth: ' + str(p4))\n",
    "\n",
    "p5 = 75          # n_estimators\n",
    "print('N_estimators: ' + str(p5))\n",
    "\n",
    "p6 = 0.5107232023187358                  # subsample\n",
    "print('Subsample: ' + str(p6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0375db3-3f54-44ae-8852-a2d349985444",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimation of predictive quality to basic XGBoost models using Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-compatibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate parameters\n",
    "aggr_y=[]\n",
    "aggr_y_pred=[]\n",
    "\n",
    "aggr_optimal_n=[]\n",
    "aggr_bias=[]\n",
    "aggr_rel_bias=[]\n",
    "aggr_rmse=[]\n",
    "aggr_rel_rmse=[]\n",
    "aggr_mse=[]\n",
    "aggr_R_square=[]\n",
    "\n",
    "exp_aggr_optimal_n=[]\n",
    "exp_aggr_bias=[]\n",
    "exp_aggr_rel_bias=[]\n",
    "exp_aggr_rmse=[]\n",
    "exp_aggr_rel_rmse=[]\n",
    "exp_aggr_mse=[]\n",
    "exp_aggr_R_square=[]\n",
    "\n",
    "aggr_Shap_values=pd.DataFrame()\n",
    "\n",
    "# Body loop\n",
    "for i in range(200):\n",
    "    \n",
    "    # Generate test and training samples\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    #exp_y_train = np.exp(y_train)\n",
    "    exp_y_test = np.exp(y_test)\n",
    "    \n",
    "    # Implementation of the scikit-learn API for XGBoost regression\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=p1, gamma=p2, learning_rate=p3, \n",
    "                          max_depth=p4, n_estimators=p5, subsample=p6, eval_metric=[\"rmse\"])\n",
    "    \n",
    "    # Fitting the model \n",
    "    xgb_model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "    \n",
    "    # learning dynamics\n",
    "    y_pred = xgb_model.predict(X_test, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    \n",
    "    exp_y_pred = np.exp(y_pred)\n",
    "    \n",
    "    # Iteration with the best result\n",
    "    optimal_n= xgb_model.best_ntree_limit-1\n",
    "    \n",
    "    # Convert data to 'array' type\n",
    "    conv_y_pred = pd.DataFrame(y_pred) # Double transformation\n",
    "    y_pred2 = conv_y_pred.values\n",
    "    y_test2 = y_test.values\n",
    "    \n",
    "    exp_y_pred2 = np.exp(y_pred2)\n",
    "    exp_y_test2 = np.exp(y_test2)\n",
    "    \n",
    "    # Intermediate results\n",
    "    n_sample = len(y_pred2)\n",
    "    y_mean_sample = y_test.sum() / n_sample\n",
    "    \n",
    "    exp_y_mean_sample = exp_y_test.sum() / n_sample\n",
    "    \n",
    "    # Calculation of bias\n",
    "    diff = y_pred2 - y_test2\n",
    "    bias = diff.sum()/n_sample\n",
    "    rel_bias = bias/y_mean_sample*100\n",
    "    \n",
    "    exp_diff = exp_y_pred2 - exp_y_test2\n",
    "    exp_bias = exp_diff.sum()/n_sample\n",
    "    exp_rel_bias = exp_bias/exp_y_mean_sample*100\n",
    "    \n",
    "    # Calculation of RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rel_rmse = rmse/y_mean_sample*100 \n",
    "    \n",
    "    exp_rmse = np.sqrt(mean_squared_error(exp_y_test, exp_y_pred))\n",
    "    exp_rel_rmse = exp_rmse/exp_y_mean_sample*100 \n",
    "    \n",
    "    # Calculation of MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)   \n",
    "    \n",
    "    exp_mse = mean_squared_error(exp_y_test, exp_y_pred)\n",
    "    \n",
    "    # Calculation of Square R\n",
    "    diff_with_mean = []\n",
    "    exp_diff_with_mean = []\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        interm = y_test2[i][0] - y_mean_sample\n",
    "        diff_with_mean.append(interm**2)\n",
    "\n",
    "    RSS = np.sum((y_test2 - y_pred2)**2)\n",
    "    TSS = (np.sum(diff_with_mean))\n",
    "    R_square = 1 - (RSS / TSS)\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        exp_interm = exp_y_test2[i][0] - exp_y_mean_sample\n",
    "        exp_diff_with_mean.append(exp_interm**2)\n",
    "\n",
    "    exp_RSS = np.sum((exp_y_test2 - exp_y_pred2)**2)\n",
    "    exp_TSS = np.sum(exp_diff_with_mean)\n",
    "    \n",
    "    exp_R_square = 1 - (exp_RSS / exp_TSS)\n",
    "    \n",
    "    #Calculation of SHAP-values\n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    vals= np.abs(shap_values).mean(0)\n",
    "    feature_importance = pd.DataFrame(list(zip(X_train.columns,vals)),columns=['Feature','Importance'])\n",
    "       \n",
    "    \n",
    "    #Add values to lists\n",
    "    aggr_y.append(y_test)\n",
    "    aggr_y_pred.append(conv_y_pred)    \n",
    "    \n",
    "    aggr_optimal_n.append(optimal_n)\n",
    "    aggr_bias.append(bias)\n",
    "    aggr_rel_bias.append(rel_bias)\n",
    "    aggr_rmse.append(rmse)\n",
    "    aggr_rel_rmse.append(rel_rmse)\n",
    "    aggr_mse.append(mse)\n",
    "    aggr_R_square.append(R_square)\n",
    "    \n",
    "    exp_aggr_bias.append(exp_bias)\n",
    "    exp_aggr_rel_bias.append(exp_rel_bias)\n",
    "    exp_aggr_rmse.append(exp_rmse)\n",
    "    exp_aggr_rel_rmse.append(exp_rel_rmse)\n",
    "    exp_aggr_mse.append(exp_mse)\n",
    "    exp_aggr_R_square.append(exp_R_square)\n",
    "    \n",
    "    aggr_Shap_values = aggr_Shap_values.append(feature_importance, ignore_index=True)\n",
    "    \n",
    "# Intermedia data to calculate CI\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9a424-ee46-4ebd-b4d6-4c61f501e36a",
   "metadata": {},
   "source": [
    "# Create plot with predicted values for test samples (after 200 iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbf70c-9f66-41e8-84d2-85fe9a959dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "pd_aggr_y = pd.concat(aggr_y, join=\"inner\")\n",
    "pd_aggr_y['Obs'] = pd_aggr_y.index\n",
    "\n",
    "pd_aggr_y_pred = pd.concat(aggr_y_pred, join=\"inner\")\n",
    "\n",
    "results_y = pd.concat([pd_aggr_y.reset_index(), pd_aggr_y_pred.reset_index()], axis=1)\n",
    "\n",
    "n_all_y = len(results_y['Obs'])\n",
    "\n",
    "stat_y = results_y.groupby(['Obs'])[predictor, 0].describe(percentiles=[.16, .5, .84])\n",
    "\n",
    "l_sd_y_pred = stat_y[0]['50%'] - stat_y[0]['16%']\n",
    "t_sd_y_pred = stat_y[0]['84%'] - stat_y[0]['50%']\n",
    "\n",
    "n_stat_y = len(stat_y[0]['50%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66ddde-77bf-47c7-be3c-fc0c4526731e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assessment of linear approximation between observations and \n",
    "predict values and estimation of parameters uncertainties for this regression\n",
    "\"\"\"\n",
    "# Load libraries elements\n",
    "from scipy.optimize import curve_fit   \n",
    "from scipy import stats\n",
    "\n",
    "# pip install uncertainties, if needed\n",
    "try:\n",
    "    import uncertainties.unumpy as unp\n",
    "    import uncertainties as unc\n",
    "except:\n",
    "    try:\n",
    "        from pip import main as pipmain\n",
    "    except:\n",
    "        from pip._internal import main as pipmain\n",
    "    pipmain(['install','uncertainties'])\n",
    "    import uncertainties.unumpy as unp\n",
    "    import uncertainties as unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469da20-59c3-4b30-9fce-35a4200242fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert input data\n",
    "x_test_conv = np.array(results_y[predictor])\n",
    "x_test_convert = np.array([None]*n_all_y)\n",
    "for i in range(n_all_y):\n",
    "    x_test_convert[i] = np.float64(x_test_conv[i])\n",
    "\n",
    "x_plot = x_test_convert\n",
    "y_plot = results_y[0]\n",
    "n = n_all_y\n",
    "\n",
    "# Function for fitting a linear curve and estimating its parameters\n",
    "def f(x_plot, a0, a1):\n",
    "    return a0 + a1 * x_plot\n",
    "\n",
    "popt, pcov = curve_fit(f, x_plot, y_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b195a6-2297-47dd-80cd-0a0a969df2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building linear a graph and calculation uncertainties\n",
    "\n",
    "# retrieve parameter values\n",
    "a0 = popt[0]\n",
    "a1 = popt[1]\n",
    "\n",
    "# compute r^2\n",
    "r2 = 1.0-(sum((y_plot-f(x_plot, a0, a1)) ** 2)/((n - 1.0)*np.var(y_plot, ddof = 1)))\n",
    "print(f'R^2: {r2:.3f}')\n",
    "\n",
    "# calculate parameter confidence interval\n",
    "a0, a1 = unc.correlated_values(popt, pcov)\n",
    "index_a0 = str(a0).find('+/-')\n",
    "index_a1 = str(a1).find('+/-')\n",
    "am_a0 = str(a0)[0:index_a0]\n",
    "ci_a0 = str(a0)[index_a0+3:]\n",
    "am_a1 = str(a1)[0:index_a1]\n",
    "ci_a1 = str(a1)[index_a1+3:]\n",
    "\n",
    "print('Uncertainty')\n",
    "print(f'a0: {am_a0}+/-{round(float(ci_a0)*(n_all_y/n_stat_y)**0.5, 3)}')\n",
    "print(f'a1: {am_a1}+/-{round(float(ci_a1)*(n_all_y/n_stat_y)**0.5, 3)}')\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize =(8, 5))\n",
    "plt.errorbar(stat_y[predictor]['mean'], stat_y[0]['50%'], yerr=[list(l_sd_y_pred), list(t_sd_y_pred)], c = 'red', fmt='o', lw=1, ms=2, label='Observations')\n",
    "plt.yscale('linear')\n",
    "\n",
    "# calculate regression confidence interval\n",
    "px = np.linspace(-0.3, max(x_plot), 100)\n",
    "py = a0 + a1 * px\n",
    "nom = unp.nominal_values(py)\n",
    "std = unp.std_devs(py)\n",
    "\n",
    "# estimation uncertenlies intervals\n",
    "def predband(x_plot, xd, yd, p, func, conf=0.95):\n",
    "    # x_plot = requested points\n",
    "    # xd = x data\n",
    "    # yd = y data\n",
    "    # p = parameters\n",
    "    # func = function name\n",
    "    alpha = 1.0 - conf    # significance\n",
    "    N = xd.size          # data sample size\n",
    "    var_n = len(p)  # number of parameters\n",
    "    # Quantile of Student's t distribution for p=(1-alpha/2)\n",
    "    q = stats.t.ppf(1.0 - alpha / 2.0, N - var_n)\n",
    "    # Stdev of an individual measurement\n",
    "    se = np.sqrt(1. / (N - var_n) * \\\n",
    "                 np.sum((yd - func(xd, *p)) ** 2))\n",
    "    # Auxiliary definitions\n",
    "    sx = (x_plot - xd.mean()) ** 2\n",
    "    sxd = np.sum((xd - xd.mean()) ** 2)\n",
    "    # Predicted values (best-fit model)\n",
    "    yp = func(x_plot, *p)\n",
    "    # Prediction band\n",
    "    dy = q * se * np.sqrt(1.0+ (1.0/N) + (sx/sxd))\n",
    "    # Upper & lower prediction bands.\n",
    "    lpb, upb = yp - dy, yp + dy\n",
    "    return lpb, upb\n",
    "\n",
    "lpb, upb = predband(px, x_plot, y_plot, popt, f, conf=0.95)\n",
    "\n",
    "# plot the regression\n",
    "plt.plot(px, nom, c='black', label='Predicted values = a0 + a1 * Observed values')\n",
    "\n",
    "# uncertainty lines (95% confidence)\n",
    "plt.plot(px, nom - 1.96 * std * (n_all_y**0.5)/(n_stat_y**0.5), c='gray',\\\n",
    "         label='95% Confidence Interval')\n",
    "plt.plot(px, nom + 1.96 * std * (n_all_y**0.5)/(n_stat_y**0.5), c='gray')\n",
    "\n",
    "# prediction band (95% confidence)\n",
    "plt.plot(px, lpb, 'k--',label='95% Prediction Band')\n",
    "plt.plot(px, upb, 'k--')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlabel('Observed values')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Empiric obs. vs Predict values', fontsize=20, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb699288-aab8-4037-a094-f6b4256a1111",
   "metadata": {},
   "source": [
    "# Uncertainty parameters and confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75292ea-e049-4588-a3b7-4f8451ab5132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intermedia ratio for calculation CI\n",
    "CIR = 2/((n_test + n_train)/n_test)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47baa86-7552-4668-813a-b77ebf3b5415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Statistic for SHAP-values\n",
    "\n",
    "number_iteration = len(aggr_Shap_values.groupby(['Feature']))\n",
    "shap_list = aggr_Shap_values.groupby(['Feature'])\n",
    "\n",
    "\n",
    "for i in range(number_iteration):\n",
    "    itr_mean = round(np.mean(list(shap_list)[i][1].iloc[0:, 1]), 3)\n",
    "    itr_Q50 = round(np.quantile(list(shap_list)[i][1].iloc[0:, 1], 0.50), 3)\n",
    "    itr_Q16 = round(np.quantile(list(shap_list)[i][1].iloc[0:, 1], 0.16), 3)\n",
    "    itr_Q84 = round(np.quantile(list(shap_list)[i][1].iloc[0:, 1], 0.84), 3)\n",
    "    itr_l = (itr_Q50 - itr_Q16)*CIR\n",
    "    l_CI = itr_Q50 - itr_l\n",
    "    itr_t = (itr_Q84 - itr_Q50)*CIR\n",
    "    t_CI = itr_Q50 + itr_t    \n",
    "    print(f\"SHAP-values for: {list(shap_list)[i][0]}: mean - {itr_mean}, median - {itr_Q50}, CI - [{round(l_CI, 3)}-{round(t_CI, 3)}]\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60b143-5bb8-488c-be52-f91b6de04071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "sns.violinplot(x=\"Importance\", y=\"Feature\", data=aggr_Shap_values, inner=\"box\", palette=\"Spectral\", order=['r2020B09', 'r2020B07', 'r2020B08', 'r2020B10', 'r2020B04'])\n",
    "sns.set(rc={'figure.figsize':(8,4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7d1ae-a2f4-4025-91b1-ec0128694191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate stat\n",
    "# optimal number of iterations\n",
    "aggr_optimal_n_mean = np.mean(aggr_optimal_n)\n",
    "aggr_optimal_n_Q025 = np.quantile(aggr_optimal_n, 0.025)\n",
    "aggr_optimal_n_Q16 = np.quantile(aggr_optimal_n, 0.16)\n",
    "aggr_optimal_n_Q50 = np.quantile(aggr_optimal_n, 0.50)\n",
    "aggr_optimal_n_Q84 = np.quantile(aggr_optimal_n, 0.84)\n",
    "aggr_optimal_n_Q975 = np.quantile(aggr_optimal_n, 0.975)\n",
    "\n",
    "# bias\n",
    "aggr_bias_mean = np.mean(aggr_bias)\n",
    "aggr_bias_Q025 = np.quantile(aggr_bias, 0.025)\n",
    "aggr_bias_Q16 = np.quantile(aggr_bias, 0.16)\n",
    "aggr_bias_Q50 = np.quantile(aggr_bias, 0.50)\n",
    "aggr_bias_Q84 = np.quantile(aggr_bias, 0.84)\n",
    "aggr_bias_Q975 = np.quantile(aggr_bias, 0.975)\n",
    "\n",
    "aggr_rel_bias_mean = np.mean(aggr_rel_bias)\n",
    "aggr_rel_bias_Q025 = np.quantile(aggr_rel_bias, 0.025)\n",
    "aggr_rel_bias_Q16 = np.quantile(aggr_rel_bias, 0.16)\n",
    "aggr_rel_bias_Q50 = np.quantile(aggr_rel_bias, 0.50)\n",
    "aggr_rel_bias_Q84 = np.quantile(aggr_rel_bias, 0.84)\n",
    "aggr_rel_bias_Q975 = np.quantile(aggr_rel_bias, 0.975)\n",
    "\n",
    "# RMSE\n",
    "aggr_rmse_mean = np.mean(aggr_rmse)\n",
    "aggr_rmse_Q025 = np.quantile(aggr_rmse, 0.025)\n",
    "aggr_rmse_Q16 = np.quantile(aggr_rmse, 0.16)\n",
    "aggr_rmse_Q50 = np.quantile(aggr_rmse, 0.50)\n",
    "aggr_rmse_Q84 = np.quantile(aggr_rmse, 0.84)\n",
    "aggr_rmse_Q975 = np.quantile(aggr_rmse, 0.975)\n",
    "\n",
    "aggr_rel_rmse_mean = np.mean(aggr_rel_rmse)\n",
    "aggr_rel_rmse_Q025 = np.quantile(aggr_rel_rmse, 0.025)\n",
    "aggr_rel_rmse_Q16 = np.quantile(aggr_rel_rmse, 0.16)\n",
    "aggr_rel_rmse_Q50 = np.quantile(aggr_rel_rmse, 0.50)\n",
    "aggr_rel_rmse_Q84 = np.quantile(aggr_rel_rmse, 0.84)\n",
    "aggr_rel_rmse_Q975 = np.quantile(aggr_rel_rmse, 0.975)\n",
    "\n",
    "# MSE\n",
    "aggr_mse_mean = np.mean(aggr_mse)\n",
    "aggr_mse_Q025 = np.quantile(aggr_mse, 0.025)\n",
    "aggr_mse_Q16 = np.quantile(aggr_mse, 0.16)\n",
    "aggr_mse_Q50 = np.quantile(aggr_mse, 0.50)\n",
    "aggr_mse_Q84 = np.quantile(aggr_mse, 0.84)\n",
    "aggr_mse_Q975 = np.quantile(aggr_mse, 0.975)\n",
    "\n",
    "# R-square\n",
    "aggr_R_square_mean = np.mean(aggr_R_square)\n",
    "aggr_R_square_Q025 = np.quantile(aggr_R_square, 0.025)\n",
    "aggr_R_square_Q16 = np.quantile(aggr_R_square, 0.16)\n",
    "aggr_R_square_Q50 = np.quantile(aggr_R_square, 0.50)\n",
    "aggr_R_square_Q84 = np.quantile(aggr_R_square, 0.84)\n",
    "aggr_R_square_Q975 = np.quantile(aggr_R_square, 0.975)\n",
    "\n",
    "# exp_bias\n",
    "exp_aggr_bias_mean = np.mean(exp_aggr_bias)\n",
    "exp_aggr_bias_Q025 = np.quantile(exp_aggr_bias, 0.025)\n",
    "exp_aggr_bias_Q16 = np.quantile(exp_aggr_bias, 0.16)\n",
    "exp_aggr_bias_Q50 = np.quantile(exp_aggr_bias, 0.50)\n",
    "exp_aggr_bias_Q84 = np.quantile(exp_aggr_bias, 0.84)\n",
    "exp_aggr_bias_Q975 = np.quantile(exp_aggr_bias, 0.975)\n",
    "\n",
    "exp_aggr_rel_bias_mean = np.mean(exp_aggr_rel_bias)\n",
    "exp_aggr_rel_bias_Q025 = np.quantile(exp_aggr_rel_bias, 0.025)\n",
    "exp_aggr_rel_bias_Q16 = np.quantile(exp_aggr_rel_bias, 0.16)\n",
    "exp_aggr_rel_bias_Q50 = np.quantile(exp_aggr_rel_bias, 0.50)\n",
    "exp_aggr_rel_bias_Q84 = np.quantile(exp_aggr_rel_bias, 0.84)\n",
    "exp_aggr_rel_bias_Q975 = np.quantile(exp_aggr_rel_bias, 0.975)\n",
    "\n",
    "# exp_RMSE\n",
    "exp_aggr_rmse_mean = np.mean(exp_aggr_rmse)\n",
    "exp_aggr_rmse_Q025 = np.quantile(exp_aggr_rmse, 0.025)\n",
    "exp_aggr_rmse_Q16 = np.quantile(exp_aggr_rmse, 0.16)\n",
    "exp_aggr_rmse_Q50 = np.quantile(exp_aggr_rmse, 0.50)\n",
    "exp_aggr_rmse_Q84 = np.quantile(exp_aggr_rmse, 0.84)\n",
    "exp_aggr_rmse_Q975 = np.quantile(exp_aggr_rmse, 0.975)\n",
    "\n",
    "exp_aggr_rel_rmse_mean = np.mean(exp_aggr_rel_rmse)\n",
    "exp_aggr_rel_rmse_Q025 = np.quantile(exp_aggr_rel_rmse, 0.025)\n",
    "exp_aggr_rel_rmse_Q16 = np.quantile(exp_aggr_rel_rmse, 0.16)\n",
    "exp_aggr_rel_rmse_Q50 = np.quantile(exp_aggr_rel_rmse, 0.50)\n",
    "exp_aggr_rel_rmse_Q84 = np.quantile(exp_aggr_rel_rmse, 0.84)\n",
    "exp_aggr_rel_rmse_Q975 = np.quantile(exp_aggr_rel_rmse, 0.975)\n",
    "\n",
    "# exp_MSE\n",
    "exp_aggr_mse_mean = np.mean(exp_aggr_mse)\n",
    "exp_aggr_mse_Q025 = np.quantile(exp_aggr_mse, 0.025)\n",
    "exp_aggr_mse_Q16 = np.quantile(exp_aggr_mse, 0.16)\n",
    "exp_aggr_mse_Q50 = np.quantile(exp_aggr_mse, 0.50)\n",
    "exp_aggr_mse_Q84 = np.quantile(exp_aggr_mse, 0.84)\n",
    "exp_aggr_mse_Q975 = np.quantile(exp_aggr_mse, 0.975)\n",
    "\n",
    "# exp_R-square\n",
    "exp_aggr_R_square_mean = np.mean(exp_aggr_R_square)\n",
    "exp_aggr_R_square_Q025 = np.quantile(exp_aggr_R_square, 0.025)\n",
    "exp_aggr_R_square_Q16 = np.quantile(exp_aggr_R_square, 0.16)\n",
    "exp_aggr_R_square_Q50 = np.quantile(exp_aggr_R_square, 0.50)\n",
    "exp_aggr_R_square_Q84 = np.quantile(exp_aggr_R_square, 0.84)\n",
    "exp_aggr_R_square_Q975 = np.quantile(exp_aggr_R_square, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235662d-34b1-4e33-80dd-bb92c07708b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output calculated stat\n",
    "print(\"Logarithmically transformed predictive values:\")\n",
    "print(f\"Aggregate optimal number of iterations statistic: \\n mean: {round(aggr_optimal_n_mean, 3)}, median: {round(aggr_optimal_n_Q50, 3)},\\n pred. interval (P(0.025)-P(0.975)): {round(aggr_optimal_n_Q025, 3)}-{round(aggr_optimal_n_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_optimal_n_Q50-((aggr_optimal_n_Q50-aggr_optimal_n_Q16)*CIR), 3)}-{round(aggr_optimal_n_Q50+((aggr_optimal_n_Q84-aggr_optimal_n_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate bias statistic: \\n mean: {round(aggr_bias_mean, 3)}, median: {round(aggr_bias_Q50, 3)},\\n pred. interval (P(0.025)-P(0.975)): {round(aggr_bias_Q025, 3)}-{round(aggr_bias_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_bias_Q50-((aggr_bias_Q50-aggr_bias_Q16)*CIR), 3)}-{round(aggr_bias_Q50+((aggr_bias_Q84-aggr_bias_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate RMSE statistic: \\n mean: {round(aggr_rmse_mean, 3)}, median: {round(aggr_rmse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(aggr_rmse_Q025, 3)}-{round(aggr_rmse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_rmse_Q50-((aggr_rmse_Q50-aggr_rmse_Q16)*CIR), 3)}-{round(aggr_rmse_Q50+((aggr_rmse_Q84-aggr_rmse_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate MSE statistic: \\n mean: {round(aggr_mse_mean, 3)}, median: {round(aggr_mse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(aggr_mse_Q025, 3)}-{round(aggr_mse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_mse_Q50-((aggr_mse_Q50-aggr_mse_Q16)*CIR), 3)}-{round(aggr_mse_Q50+((aggr_mse_Q84-aggr_mse_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate R^2 statistic: \\n mean: {round(aggr_R_square_mean, 3)}, median: {round(aggr_R_square_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(aggr_R_square_Q025, 3)}-{round(aggr_R_square_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_R_square_Q50-((aggr_R_square_Q50-aggr_R_square_Q16)*CIR), 3)}-{round(aggr_R_square_Q50+((aggr_R_square_Q84-aggr_R_square_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate bias(%) statistic: \\n mean: {round(aggr_rel_bias_mean, 3)}, median: {round(aggr_rel_bias_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(aggr_rel_bias_Q025, 3)}-{round(aggr_rel_bias_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_rel_bias_Q50-((aggr_rel_bias_Q50-aggr_rel_bias_Q16)*CIR), 3)}-{round(aggr_rel_bias_Q50+((aggr_rel_bias_Q84-aggr_rel_bias_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate RMSE(%) statistic: \\n mean: {round(aggr_rel_rmse_mean, 3)}, median: {round(aggr_rel_rmse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(aggr_rel_rmse_Q025, 3)}-{round(aggr_rel_rmse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(aggr_rel_rmse_Q50-((aggr_rel_rmse_Q50-aggr_rel_rmse_Q16)*CIR), 3)}-{round(aggr_rel_rmse_Q50+((aggr_rel_rmse_Q84-aggr_rel_rmse_Q50)*CIR), 3)} \\n***\")\n",
    "\n",
    "print(\"\\nTransformed predictive values into the primary state:\")\n",
    "print(f\"Aggregate bias statistic: \\n mean: {round(exp_aggr_bias_mean, 3)}, median: {round(exp_aggr_bias_Q50, 3)},\\n pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_bias_Q025, 3)}-{round(exp_aggr_bias_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_bias_Q50-((exp_aggr_bias_Q50-exp_aggr_bias_Q16)*CIR), 3)}-{round(exp_aggr_bias_Q50+((exp_aggr_bias_Q84-exp_aggr_bias_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate RMSE statistic: \\n mean: {round(exp_aggr_rmse_mean, 3)}, median: {round(exp_aggr_rmse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_rmse_Q025, 3)}-{round(exp_aggr_rmse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_rmse_Q50-((exp_aggr_rmse_Q50-exp_aggr_rmse_Q16)*CIR), 3)}-{round(exp_aggr_rmse_Q50+((exp_aggr_rmse_Q84-exp_aggr_rmse_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate MSE statistic: \\n mean: {round(exp_aggr_mse_mean, 3)}, median: {round(exp_aggr_mse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_mse_Q025, 3)}-{round(exp_aggr_mse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_mse_Q50-((exp_aggr_mse_Q50-exp_aggr_mse_Q16)*CIR), 3)}-{round(exp_aggr_mse_Q50+((exp_aggr_mse_Q84-exp_aggr_mse_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate R^2 statistic: \\n mean: {round(exp_aggr_R_square_mean, 3)}, median: {round(exp_aggr_R_square_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_R_square_Q025, 3)}-{round(exp_aggr_R_square_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_R_square_Q50-((exp_aggr_R_square_Q50-exp_aggr_R_square_Q16)*CIR), 3)}-{round(exp_aggr_R_square_Q50+((exp_aggr_R_square_Q84-exp_aggr_R_square_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate bias(%) statistic: \\n mean: {round(exp_aggr_rel_bias_mean, 3)}, median: {round(exp_aggr_rel_bias_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_rel_bias_Q025, 3)}-{round(exp_aggr_rel_bias_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_rel_bias_Q50-((exp_aggr_rel_bias_Q50-exp_aggr_rel_bias_Q16)*CIR), 3)}-{round(exp_aggr_rel_bias_Q50+((exp_aggr_rel_bias_Q84-exp_aggr_rel_bias_Q50)*CIR), 3)} \\n***\")\n",
    "print(f\"Aggregate RMSE(%) statistic: \\n mean: {round(exp_aggr_rel_rmse_mean, 3)}, median: {round(exp_aggr_rel_rmse_Q50, 3)}, pred. interval (P(0.025)-P(0.975)): {round(exp_aggr_rel_rmse_Q025, 3)}-{round(exp_aggr_rel_rmse_Q975, 3)}, \\n conf. interval (p-value=0.95): {round(exp_aggr_rel_rmse_Q50-((exp_aggr_rel_rmse_Q50-exp_aggr_rel_rmse_Q16)*CIR), 3)}-{round(exp_aggr_rel_rmse_Q50+((exp_aggr_rel_rmse_Q84-exp_aggr_rel_rmse_Q50)*CIR), 3)} \\n***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b7af7-ad5a-4faa-943b-8db18d61a1a9",
   "metadata": {},
   "source": [
    "# Analysis of residuals distributions dependents on empirical values of predicted parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d12b6-a713-4178-bcd8-1d822ef16f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "results_y['T_exp_empiric'] = np.exp(results_y[predictor])\n",
    "results_y['T_exp_pred'] = np.exp(results_y[0])\n",
    "\n",
    "results_y['ln_residuals'] = results_y[predictor] - results_y[0]\n",
    "results_y['T_exp_residuals'] = results_y['T_exp_empiric'] - results_y['T_exp_pred']\n",
    "results_y['ln_residuals_%'] =  results_y['ln_residuals'] / results_y[predictor] * 100\n",
    "results_y['T_exp_residuals_%'] = results_y['T_exp_residuals'] / results_y['T_exp_empiric'] * 100\n",
    "\n",
    "# plt.hist(results_y[predictor])\n",
    "# plt.hist(results_y[predictor])\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].hist(results_y[predictor])\n",
    "axs[0].set_title('Logarithmically transformed data')\n",
    "axs[1].hist(results_y['T_exp_empiric'], log=True)\n",
    "axs[1].set_title('In typical state')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd577d-e61a-411c-82d2-bf17da5fa6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPORTANT  - added your intervals\n",
    "ln_intervals = pd.cut(results_y[predictor], [0, 1, 2, 3, 4])\n",
    "T_exp_intervals = pd.cut(results_y['T_exp_empiric'], [0, 5, 10, 20, 30, 50, 100])\n",
    "\n",
    "ln_grouped = results_y.groupby(ln_intervals)\n",
    "T_exp_grouped = results_y.groupby(T_exp_intervals) \n",
    "\n",
    "max_obs_in_group = len(results_y)    \n",
    "    \n",
    "pre_ln = pd.DataFrame()\n",
    "pre_T_exp = pd.DataFrame()\n",
    "pre_rel_ln = pd.DataFrame()\n",
    "pre_rel_T_exp = pd.DataFrame()\n",
    "\n",
    "pre_ln = pre_ln.assign(row_number=range(max_obs_in_group))\n",
    "pre_T_exp = pre_T_exp.assign(row_number=range(max_obs_in_group))\n",
    "pre_rel_ln = pre_rel_ln.assign(row_number=range(max_obs_in_group))\n",
    "pre_rel_T_exp = pre_rel_T_exp.assign(row_number=range(max_obs_in_group))\n",
    "\n",
    "for i in range(len(list(ln_grouped))):\n",
    "    pre_ln[list(ln_grouped)[i][0]] = pd.Series(list(ln_grouped)[i][1]['ln_residuals'])\n",
    "    pre_rel_ln[list(ln_grouped)[i][0]] = pd.Series(list(ln_grouped)[i][1]['ln_residuals_%'])\n",
    "\n",
    "for i in range(len(list(T_exp_grouped))):\n",
    "    pre_T_exp[list(T_exp_grouped)[i][0]] = pd.Series(list(T_exp_grouped)[i][1]['T_exp_residuals'])\n",
    "    pre_rel_T_exp[list(T_exp_grouped)[i][0]] = pd.Series(list(T_exp_grouped)[i][1]['T_exp_residuals_%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1103ca-5324-458a-9618-f62159875ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots of residuals versus predicted values\n",
    "sns.set(font_scale=1.2)\n",
    "sns.violinplot(data=pre_ln[list(pre_ln.columns)[1:]], inner=\"box\", palette=\"Spectral\")\n",
    "sns.set(rc={'figure.figsize':(8,4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c171d95-9793-419d-923f-fd6eabc74740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots of relative residuals (in %) versus predicted values\n",
    "sns.set(font_scale=1.2)\n",
    "sns.violinplot(data=pre_rel_ln[(pre_rel_ln.columns)[1:]], inner=\"box\", palette=\"Spectral\")\n",
    "sns.set(rc={'figure.figsize':(8,4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ed1fb-1d63-45ac-bbd5-96c462e95a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots of residuals versus predicted values\n",
    "sns.set(font_scale=1.2)\n",
    "sns.violinplot(data=pre_T_exp[list(pre_T_exp.columns)[1:]], inner=\"box\", palette=\"Spectral\")\n",
    "sns.set(rc={'figure.figsize':(8,4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e518d57-5a48-45e1-bd22-dd02a8547a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots of relative residuals (in %) versus predicted values\n",
    "sns.set(font_scale=1.2)\n",
    "sns.violinplot(data=pre_rel_T_exp[(pre_rel_T_exp.columns)[1:]], inner=\"box\", palette=\"Spectral\")\n",
    "sns.set(rc={'figure.figsize':(8,4)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf53f5-d952-4083-b649-6623eb9cff50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Creating final XGBoost model using all obs. for its training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff63c8d-c2b7-4ff7-afc5-ab3bb704cb62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate test and training samples\n",
    "f_X_train = X\n",
    "f_y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb987ef5-b844-4717-ab1c-ad16ed29f932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementation of the scikit-learn API for XGBoost regression\n",
    "f_xgb_model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=p1, gamma=p2, learning_rate=p3, \n",
    "                          max_depth=p4, n_estimators=p5, subsample=p6, eval_metric=[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb95ed0-9b52-4e5b-a1b9-668ef9dd82e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting the model \n",
    "f_xgb_model.fit(f_X_train, f_y_train, early_stopping_rounds=20, eval_set=[(f_X_train, f_y_train)])\n",
    "# learning dynamics\n",
    "f_y_pred = f_xgb_model.predict(f_X_train, ntree_limit=f_xgb_model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed571ed-503c-45a2-9124-ed98726ca32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iteration with the best result\n",
    "f_optimal_n = f_xgb_model.best_ntree_limit-1\n",
    "print(\"The best iteration: \" + str(f_optimal_n))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b95834-50b8-44cb-9918-11bac2d55f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing data for building a learning graph\n",
    "f_results = f_xgb_model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfc6cd-0722-451a-bfef-709de8b79548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning curves for the XGBoost model \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(f_results['validation_0']['rmse'], label='Train')\n",
    "plt.axvline(f_optimal_n, color=\"gray\", label=\"Optimal iteration\")\n",
    "plt.xlabel(\"Boosting iteration, pcs\")\n",
    "plt.ylabel(\"Loss functions - RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53427d9-745d-47fb-b43b-cdc2b6a768d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimation of a final XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b46480-aaa6-48a3-82e0-3f552dd428af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data to 'array' type\n",
    "f_conv_y_pred = pd.DataFrame(f_y_pred) # Double transformation\n",
    "f_y_pred2 = f_conv_y_pred.values\n",
    "f_y_train2 = f_y_train.values\n",
    "\n",
    "# Intermediate results\n",
    "f_n_sample = len(f_y_pred2)\n",
    "f_main_sample = f_y_train.sum() / f_n_sample\n",
    "\n",
    "# Calculation of bias\n",
    "f_diff = f_y_pred2 - f_y_train2\n",
    "f_bias = f_diff.sum()/f_n_sample\n",
    "print('Bias: %.1f' % (f_bias))\n",
    "print(\"Relative bias(%):\", \" %.2f\" % (f_bias/f_main_sample*100))\n",
    "\n",
    "# Calculation of RMSE\n",
    "f_rmse = np.sqrt(mean_squared_error(f_y_train, f_y_pred))\n",
    "print(\"RMSE: %.1f\" % (f_rmse))\n",
    "print(\"Relative RMSE(%):\", \" %.3f\" % (f_rmse/f_main_sample*100))\n",
    "\n",
    "# Calculation of MSE\n",
    "f_mse = mean_squared_error(f_y_train, f_y_pred)\n",
    "print(\"MSE: %.1f\" % (f_mse))\n",
    "\n",
    "# Calculation of Square R\n",
    "f_R_square = r2_score(f_y_train, f_y_pred)\n",
    "print(\"R square: %.1f%%\" % (f_R_square * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed9699-0053-454f-ae03-8eb7daa29bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert input data\n",
    "f_x_train_conv = np.array(f_y_train)\n",
    "f_x_train_convert = np.array([None]*f_n_sample)\n",
    "for i in range(f_n_sample):\n",
    "    f_x_train_convert[i] = float(f_x_train_conv[i])\n",
    "\n",
    "x_plot = f_x_train_convert\n",
    "y_plot = f_y_pred\n",
    "n = len(y_plot)\n",
    "\n",
    "# Function for fitting a linear curve and estimating its parameters\n",
    "def f(x_plot, a0, a1):\n",
    "    return a0 + a1 * x_plot\n",
    "\n",
    "popt, pcov = curve_fit(f, x_plot, y_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8771119-24ca-417b-a14c-23776ef9d936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building linear a graph and calculation uncertainties\n",
    "\n",
    "# retrieve parameter values\n",
    "a0 = popt[0]\n",
    "a1 = popt[1]\n",
    "\n",
    "# compute r^2\n",
    "r2 = 1.0-(sum((y_plot-f(x_plot, a0, a1)) ** 2)/((n - 1.0)*np.var(y_plot, ddof = 1)))\n",
    "print(f'R^2: {r2:.3f}')\n",
    "\n",
    "# calculate parameter confidence interval\n",
    "a0, a1 = unc.correlated_values(popt, pcov)\n",
    "print('Uncertainty')\n",
    "print('a0: ' + str(a0))\n",
    "print('a1: ' + str(a1))\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize =(8, 5))\n",
    "plt.scatter(x_plot, y_plot, s = 10, c = 'red', label='Observations', marker = 'o')\n",
    "plt.yscale('linear')\n",
    "\n",
    "# estimation uncertenlies intervals\n",
    "def predband(x_plot, xd, yd, p, func, conf=0.95):\n",
    "    # x_plot = requested points\n",
    "    # xd = x data\n",
    "    # yd = y data\n",
    "    # p = parameters\n",
    "    # func = function name\n",
    "    alpha = 1.0 - conf    # significance\n",
    "    N = xd.size          # data sample size\n",
    "    var_n = len(p)  # number of parameters\n",
    "    # Quantile of Student's t distribution for p=(1-alpha/2)\n",
    "    q = stats.t.ppf(1.0 - alpha / 2.0, N - var_n)\n",
    "    # Stdev of an individual measurement\n",
    "    se = np.sqrt(1. / (N - var_n) * \\\n",
    "                 np.sum((yd - func(xd, *p)) ** 2))\n",
    "    # Auxiliary definitions\n",
    "    sx = (x_plot - xd.mean()) ** 2\n",
    "    sxd = np.sum((xd - xd.mean()) ** 2)\n",
    "    # Predicted values (best-fit model)\n",
    "    yp = func(x_plot, *p)\n",
    "    # Prediction band\n",
    "    dy = q * se * np.sqrt(1.0+ (1.0/N) + (sx/sxd))\n",
    "    # Upper & lower prediction bands.\n",
    "    lpb, upb = yp - dy, yp + dy\n",
    "    return lpb, upb\n",
    "\n",
    "# calculate regression confidence interval\n",
    "px = np.linspace(-0.3, max(x_plot), 100)\n",
    "py = a0 + a1 * px\n",
    "nom = unp.nominal_values(py)\n",
    "std = unp.std_devs(py)\n",
    "\n",
    "lpb, upb = predband(px, x_plot, y_plot, popt, f, conf=0.95)\n",
    "\n",
    "# plot the regression\n",
    "plt.plot(px, nom, c='black', label='Predicted values = a0 + a1 * Observed values')\n",
    "\n",
    "# uncertainty lines (95% confidence)\n",
    "plt.plot(px, nom - 1.96 * std, c='gray',\\\n",
    "         label='95% Confidence Interval')\n",
    "plt.plot(px, nom + 1.96 * std, c='gray')\n",
    "\n",
    "# prediction band (95% confidence)\n",
    "plt.plot(px, lpb, 'k--',label='95% Prediction Band')\n",
    "plt.plot(px, upb, 'k--')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlabel('Observed values')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Empiric obs. vs Predict values', fontsize=20, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577288a-384b-4c13-b482-7bb6cdaf6391",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Analisys output final model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b001669-2db4-4104-8b2b-4e0b408a98eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the Decision tree (individual obs.)\n",
    "xgb.plot_tree(f_xgb_model, num_trees=3) # \"num_trees\" is number of obs.\n",
    "plt.rcParams['figure.figsize'] = [30, 45]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f3797-4c41-4655-8982-6965eb824c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importance is calculated as: ”weight” is the number of times a feature appears in a trees\n",
    "xgb.plot_importance(f_xgb_model, importance_type='weight')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb02a77-1ec8-40f3-b73f-733c8f787022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importance is calculated as: ”gain” is the average gain of splits which use the feature\n",
    "# Get the booster from the xgbmodel\n",
    "booster = f_xgb_model.get_booster()\n",
    "\n",
    "# Get the importance dictionary (by gain) from the booster\n",
    "importance = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "# make your changes\n",
    "for key in importance.keys():\n",
    "    importance[key] = round(importance[key], 1)\n",
    "\n",
    "# provide the importance dictionary to the plotting function\n",
    "ax = xgb.plot_importance(importance, importance_type='gain', show_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bc2c1-cf83-4201-ab40-568820fdbb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importance is calculated as: \"cover” is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n",
    "# Get the booster from the xgbmodel\n",
    "booster = f_xgb_model.get_booster()\n",
    "\n",
    "# Get the importance dictionary (by gain) from the booster\n",
    "importance = booster.get_score(importance_type=\"cover\")\n",
    "\n",
    "# make your changes\n",
    "for key in importance.keys():\n",
    "    importance[key] = round(importance[key],1)\n",
    "\n",
    "# provide the importance dictionary to the plotting function\n",
    "ax = xgb.plot_importance(importance, importance_type='cover', show_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7fcba-d36d-4853-b935-5e2daaa382c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Est. of SHAP values for final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2394f0-7116-4a56-a33f-f5edcef1240c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Сreate an уxplainer object\n",
    "explainer = shap.TreeExplainer(f_xgb_model)\n",
    "shap_values = explainer.shap_values(f_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c17f8c-976d-48d4-8344-9cd3443949f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean absolute value of the SHAP values for each feature \n",
    "shap.summary_plot(shap_values, f_X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dd36e-a682-4644-af4d-3af1b6594dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graph that summarises the effects of all the features\n",
    "shap.summary_plot(shap_values, f_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88db842-14ee-4576-a0f6-d1a2724c2ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dependence scatter plot to show the effect of a single feature across the whole dataset\n",
    "for name in X_train.columns:\n",
    "    shap.dependence_plot(name, shap_values, f_X_train, color=shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423445b-18e5-4c93-b0f5-e0741537e085",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *Save XGBoost model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304734a-30cc-4c97-a008-235bd15577b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "f_xgb_model.save_model(\"../05_output_data/XGBoost_models/10_LN_Tag_Sr_SENTINEL_2020_XGB_model_2024_final.json\")\n",
    "# Save to text format\n",
    "f_xgb_model.save_model(\"../05_output_data/XGBoost_models/10_LN_Tag_Sr_SENTINEL_2020_XGB_model_2024_final.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97b41a-f21b-41f6-aa74-c26c248ba46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
